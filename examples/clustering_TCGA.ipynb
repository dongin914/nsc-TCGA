{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../DeepSurvivalMachines/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.load('../ProcessedData/GeneCount.npy',allow_pickle=True)\n",
    "t = np.load('../ProcessedData/TTE.npy',allow_pickle=True)\n",
    "e = np.load('../ProcessedData/Event.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "np.random.seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "horizons = [0.75]\n",
    "times = np.quantile(t[e!=0], horizons).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At time 7.23\n",
      "\t 64.92 % observed risk 0\n",
      "\t 13.87 % observed risk 1\n"
     ]
    }
   ],
   "source": [
    "# Display the percentage of observed event at different time horizon\n",
    "for time in times:\n",
    "    print('At time {:.2f}'.format(time))\n",
    "    for risk in np.unique(e):\n",
    "        print('\\t {:.2f} % observed risk {}'.format(100 * ((e == risk) & (t < time)).mean(), risk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x_train, x_test, t_train, t_test, e_train, e_test = train_test_split(x, t, e, test_size = 0.2, random_state = 42)\n",
    "x_train, x_val, t_train, t_val, e_train, e_val = train_test_split(x_train, t_train, e_train, test_size = 0.2, random_state = 42)\n",
    "x_dev, x_val, t_dev, t_val, e_dev, e_val = train_test_split(x_val, t_val, e_val, test_size = 0.5, random_state = 42)\n",
    "\n",
    "minmax = lambda x: x / t_train.max() # Enforce to be inferior to 1\n",
    "t_train_ddh = minmax(t_train)\n",
    "t_dev_ddh = minmax(t_dev)\n",
    "t_val_ddh = minmax(t_val)\n",
    "times_ddh = minmax(np.array(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [[50], [50, 50], [50, 50, 50], [100], [100, 100], [100, 100, 100]]\n",
    "param_grid = {\n",
    "            'learning_rate' : [1e-3, 1e-4],\n",
    "            'layers_surv': layers,\n",
    "            'k': [2],\n",
    "            'representation': [50, 100],\n",
    "            'layers' : layers,\n",
    "            'act': ['Tanh'],\n",
    "            'batch': [100, 250],\n",
    "            }\n",
    "params = ParameterSampler(param_grid, 280 , random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nsc import NeuralSurvivalCluster\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.431: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  3.31it/s]\n",
      "Loss: 0.395: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 11.82it/s]\n",
      "Loss: 0.154: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  7.16it/s]\n",
      "Loss: 0.169: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.87it/s]\n",
      "Loss: 0.431: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 11.22it/s]\n",
      "Loss: 0.153: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.36it/s]\n",
      "Loss: 0.977: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  3.80it/s]\n",
      "Loss: 0.486:  60%|██████████████████████████████████████████                            | 6/10 [00:00<00:00, 10.20it/s]"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for param in params:\n",
    "    model = NeuralSurvivalCluster(layers = param['layers'], act = param['act'], k = param['k'],\n",
    "                                layers_surv = param['layers_surv'], representation = param['representation'])\n",
    "    # The fit method is called to train the model\n",
    "    model.fit(x_train, t_train_ddh, e_train, n_iter = 10, bs = param['batch'],\n",
    "            lr = param['learning_rate'], val_data = (x_dev, t_dev_ddh, e_dev))\n",
    "    nll = model.compute_nll(x_val, t_val_ddh, e_val)\n",
    "    if not(np.isnan(nll)):\n",
    "        models.append([nll, model])\n",
    "    else:\n",
    "        print(\"WARNING: Nan Value Observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = min(models, key = lambda x: x[0])\n",
    "model = best_model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_risk = model.predict_risk(x_test, times_ddh.tolist())\n",
    "out_survival= model.predict_survival(x_test, times_ddh.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import concordance_index_ipcw, brier_score, cumulative_dynamic_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_train = np.array([(e_train[i] == 1, t_train[i]) for i in range(len(e_train))],\n",
    "                 dtype = [('e', bool), ('t', float)])\n",
    "et_test = np.array([(e_test[i] == 1, t_test[i]) for i in range(len(e_test))],\n",
    "                 dtype = [('e', bool), ('t', float)])\n",
    "selection = (t_test < t_train.max()) | (e_test == 0)\n",
    "\n",
    "cis = []\n",
    "for i, _ in enumerate(times):\n",
    "    cis.append(concordance_index_ipcw(et_train, et_test[selection], out_risk[:, i][selection], times[i])[0])\n",
    "brs = brier_score(et_train, et_test[selection], out_survival[selection], times)[1]\n",
    "roc_auc = []\n",
    "for i, _ in enumerate(times):\n",
    "    roc_auc.append(cumulative_dynamic_auc(et_train, et_test[selection], out_risk[:, i][selection], times[i])[0])\n",
    "for horizon in enumerate(horizons):\n",
    "    print(f\"For {horizon[1]} quantile,\")\n",
    "    print(\"TD Concordance Index:\", cis[horizon[0]])\n",
    "    print(\"Brier Score:\", brs[horizon[0]])\n",
    "    print(\"ROC AUC \", roc_auc[horizon[0]][0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_cluster = np.quantile(t, np.linspace(0, 1, 100))\n",
    "clusters = model.survival_cluster(minmax(times_cluster).tolist(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(clusters, index = times_cluster).plot()\n",
    "plt.grid(alpha = 0.3)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title = 'Clusters')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Survival Probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "times_cluster = np.quantile(t, np.linspace(0, 1, 100))\n",
    "clusters = model.survival_cluster(minmax(times_cluster).tolist(), 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for cluster in range(clusters.shape[1]):\n",
    "    cluster_survival = clusters[:, cluster]\n",
    "    step_times = np.repeat(times_cluster, 2)[1:]\n",
    "    step_survival = np.repeat(cluster_survival, 2)[:-1]\n",
    "    ax.plot(step_times, step_survival, label=f'Cluster {cluster}')\n",
    "\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(title='Clusters')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Survival Probability')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.survival_cluster(minmax(times_cluster).tolist(), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1b50223f39b64c0c24545f474e3e7d2d3b4b121fe045100fc03a3926bb649af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
